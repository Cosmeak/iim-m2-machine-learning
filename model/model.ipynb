{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.classification import Accuracy\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.utils.data as data\n",
    "\n",
    "class VGGLightning(pl.LightningModule):\n",
    "    def __init__(self, train_set, valid_set, batch_size=128, num_classes=10):\n",
    "        super(VGGLightning, self).__init__()\n",
    "        \n",
    "        # Define VGG architecture for 28x28 MNIST images (grayscale)\n",
    "        self.features = nn.Sequential(\n",
    "            # First block: Conv -> Conv -> MaxPool\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),  # 1 input channel (grayscale), 64 output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output size: 64x14x14\n",
    "\n",
    "            # Second block: Conv -> Conv -> MaxPool\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # 64 input channels, 128 output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output size: 128x7x7\n",
    "\n",
    "            # Third block: Conv -> Conv -> Conv -> MaxPool\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # 128 input channels, 256 output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output size: 256x3x3\n",
    "\n",
    "            # Fourth block: Conv -> Conv -> Conv -> MaxPool\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),  # 256 input channels, 512 output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Output size: 512x1x1\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 1 * 1, 4096),  # Adjusted to 512 * 1 * 1 based on input size\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)  # Output 10 classes for MNIST digits\n",
    "        )\n",
    "\n",
    "        # Metric for accuracy\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "        # Define batch_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Define datasets\n",
    "        self.train_set = train_set\n",
    "        self.valid_set = valid_set\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output from convolutional layers\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, num_workers=11, persistent_workers=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Extract input images and labels\n",
    "        images, labels = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self(images)\n",
    "        \n",
    "        # Calculate loss (cross-entropy loss)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        \n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_set, batch_size=self.batch_size, num_workers=11, persistent_workers=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = self.accuracy(outputs, labels)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_acc', acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = self.accuracy(outputs, labels)\n",
    "        \n",
    "        self.log('test_loss', loss, on_epoch=True)\n",
    "        self.log('test_acc', acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=0.001)\n",
    "        scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1),\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "        \n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        images, _ = batch # Don't need the labels in prediction\n",
    "        outputs = self(images)\n",
    "        # Get the predicted class with the highest prob\n",
    "        _, predicted = torch.max(outputs, 1) \n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define datasets\n",
    "training_set = datasets.MNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(training_set) * 0.8)\n",
    "valid_set_size = len(training_set) - train_set_size\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, valid_set = data.random_split(training_set, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "# Initialize model \n",
    "model = VGGLightning(train_set, valid_set)\n",
    "\n",
    "# Create a Trainer object\n",
    "trainer = pl.Trainer(default_root_dir=\"checkpoints/\", callbacks=[pl.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\")], max_epochs=-1)\n",
    "\n",
    "# Tune batch scale\n",
    "# tuner = pl.tuner.Tuner(trainer)\n",
    "# tuner.scale_batch_size(model)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model)\n",
    "\n",
    "# Validate training\n",
    "test_set = datasets.MNIST(root=\"data\", train=False, download=True, transform=ToTensor())\n",
    "test_dataloader = DataLoader(test_set, num_workers=11, persistent_workers=True)\n",
    "trainer.test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = trainer.predict(model, test_dataloader)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
